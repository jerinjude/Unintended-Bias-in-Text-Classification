{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install contractions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install fastext","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing Libraries ###","metadata":{}},{"cell_type":"code","source":"import regex as re\nimport nltk\nimport string\nimport fasttext\nimport contractions\nimport nltk\nnltk.download('omw-1.4')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer\nimport emoji\nimport matplotlib as plt\nfrom textblob import TextBlob\nfrom tqdm.notebook import tqdm_notebook\nimport pandas as pd\nimport numpy as np\n#plt.pyplot.xticks(rotation=70)\npd.options.mode.chained_assignment = None\npd.set_option('display.max_colwidth', 100)\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:39:02.867329Z","iopub.execute_input":"2022-08-11T04:39:02.869530Z","iopub.status.idle":"2022-08-11T04:39:02.885484Z","shell.execute_reply.started":"2022-08-11T04:39:02.869450Z","shell.execute_reply":"2022-08-11T04:39:02.883910Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"markdown","source":"### Reading File ###","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Duplicates ###","metadata":{}},{"cell_type":"code","source":"df=df.drop_duplicates(subset=['comment_text','target'])\ndf['comment_text']=df['comment_text'].astype('string')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_model = \"../input/lid-language/lid.176.bin\" \nmodel = fasttext.load_model(pretrained_model)\nlangs = []\nfor sent in df['comment_text']:\n    lang = model.predict(sent)[0]\n    langs.append(str(lang)[11:13])\ndf['langs'] = langs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['langs']=df['langs'].astype('string')\ndf=df[df['langs']=='en']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments=pd.DataFrame(df['comment_text'])\ncomments.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dividing dataframe ###","metadata":{}},{"cell_type":"code","source":"df_part1=df.iloc[:300000,]\ndf_part2=df.iloc[300000:600000,]\ndf_part3=df.iloc[600000:900000,]\ndf_part4=df.iloc[900000:1200000,]\ndf_part5=df.iloc[1200000:1500000,]\ndf_part6=df.iloc[1500000:,]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text cleaning function ###","metadata":{}},{"cell_type":"code","source":"def preprocess_text(text):\n    words=[contractions.fix(word) for word in text.split()]\n    text=(' ').join(words)\n    text=text.lower()\n    #print(text)\n    text=re.sub(r'https:\\S*','',text) #http\n    text=re.sub(r'sh\\*tty','shitty',text) \n    text=re.sub(r'(ha){2,}','laughing',text) #hahaha\n    text=re.sub(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+',\"\",text) #email\n    text=re.sub(r'\\S+\\.com','',text) #.com\n    emoji.demojize(text, delimiters=(\"\", \"\"))\n    text=re.sub(r'\\n','',text)\n    text=re.sub(r'\\-+','',text) #--\n    text=re.sub(r'btw','by the way',text)\n    text=re.sub(r'\\bla\\b','',text) #la la la\n    text=re.sub(r'<.*?>','',text) #<Fgfdrghd>\n    text=re.sub(r'\\[','',text) \n    text=re.sub(r'\\]','',text)\n    text=re.sub(r'hm+','',text) #hmmmm\n    text=re.sub(r'mm+','',text) #mmmmm\n    text=re.sub(r's\\*\\*\\*\\*-storm','shit-storm',text)\n    text=re.sub(r'p\\*\\*\\*y','pussy',text)\n    text=re.sub(r'f\\*cks','fucks',text)\n    text=re.sub(r'f\\*\\*k','fucking',text)\n    text=re.sub(r'a\\S*\\**\\S*\\*hole','asshole',text)\n    text=re.sub(r'a\\S*\\**\\S*\\*e','asshole',text)\n    text=re.sub(r'c\\*\\*nt','cunt',text)\n    text=re.sub(r'moth\\S+\\*+s','mother fuckers',text)\n    text=re.sub(r'moth\\S+\\*+r','mother fucker',text)\n    text=re.sub(r'b\\**\\S*\\**h','bitch',text)\n    text=re.sub(r'n\\**\\S*\\**r','nigger',text)\n    text=re.sub(r'b\\*+d','bastard',text)\n    text=re.sub(r'd\\*vilish','devilish',text)\n    text=re.sub(r'sh\\*t','shit',text)\n    text=re.sub(r'f\\*+n','fucking',text)\n    text=re.sub(r'fuckin','fucking',text)\n    text=re.sub(r'dipsh\\*t','dipshit',text)\n    text=re.sub(r'motha\\*+','mother fucking',text)\n    text=re.sub(r'stoopiidd','stupid',text)\n    text=re.sub(r'p\\*+s','pussies',text)\n    text=re.sub(r'd\\*ck','dick',text)\n    text=re.sub(r'f\\**\\S*\\**face','fuck face',text)\n    text=re.sub(r'lo+ng','long',text)\n    text=re.sub(r'sh\\*\\*pile','shitpile',text)\n    text=re.sub(r'b\\*lls','balls',text)\n    text=re.sub(r'pr\\*ck','prick',text)\n    text=re.sub(r'bats\\*\\*t','batshit',text)\n    text=re.sub(r'\\'em','asshole',text)\n    text=re.sub(r'h\\*ll','hell',text)\n    text=re.sub(r'gtfo','get the fuck off',text)\n    text=re.sub(r'fellas','fellows',text)\n    text=re.sub(r'tw\\*t','twat',text)\n    text=re.sub(r'wh\\*re','whore',text)\n    text=re.sub(r'imho','in my humble opinion',text)\n    text=re.sub(r'f\\*\\&\\^\\?','fuck',text)\n    text=re.sub(r'(\\w)\\1{3,}',r'\\1',text)   \n    text=re.sub(r'[\\!\\\"\\#\\$\\%\\&\\\\\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]','',text)\n    text=re.sub(r'[0-9]','',text)\n    text=re.sub(r'\\s\\w\\s','',text)\n    \n    text=str(TextBlob(text).correct())\n    stop_words = set(stopwords.words('english'))\n    words=[word for word in text.split() if word not in stop_words]\n    text=(' ').join(words)\n    pos=nltk.pos_tag(word_tokenize(text)) \n    def get_wordnet_pos(tag):\n        if tag.startswith('J'):\n            return wordnet.ADJ\n        elif tag.startswith('V'):\n            return wordnet.VERB\n        elif tag.startswith('N'):\n            return wordnet.NOUN\n        elif tag.startswith('R'):\n            return wordnet.ADV\n        else:\n            return wordnet.NOUN\n    wn_pos=[(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in pos]\n    wnl = WordNetLemmatizer()\n    words=[wnl.lemmatize(word, tag) for word, tag in wn_pos]\n    text=(' ').join(words)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:03:57.188628Z","iopub.execute_input":"2022-08-11T04:03:57.189146Z","iopub.status.idle":"2022-08-11T04:03:57.218924Z","shell.execute_reply.started":"2022-08-11T04:03:57.189106Z","shell.execute_reply":"2022-08-11T04:03:57.217086Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"tqdm_notebook.pandas()\ndf_part1['proceesed_comment']=df_part1['comment_text'].apply(prepocess_text)\ndf_part2['proceesed_comment']=df_part2['comment_text'].apply(prepocess_text)\ndf_part3['proceesed_comment']=df_part3['comment_text'].apply(prepocess_text)\ndf_part4['proceesed_comment']=df_part4['comment_text'].apply(prepocess_text)\ndf_part5['proceesed_comment']=df_part5['comment_text'].apply(prepocess_text)\ndf_part6['processed_comment']=df_part6['comment_text'].progress_apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2022-08-11T04:16:34.756456Z","iopub.execute_input":"2022-08-11T04:16:34.757315Z","iopub.status.idle":"2022-08-11T04:17:35.389486Z","shell.execute_reply.started":"2022-08-11T04:16:34.757258Z","shell.execute_reply":"2022-08-11T04:17:35.387539Z"},"trusted":true},"execution_count":130,"outputs":[]}]}